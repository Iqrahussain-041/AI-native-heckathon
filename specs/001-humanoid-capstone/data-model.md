# Data Model: Autonomous Humanoid Robot Capstone

**Date**: 2025-12-05  
**Phase**: Phase 1 (Design & Contracts)  
**Purpose**: Define entities, data flows, and state machines for the humanoid robot system

---

## Core Entities

### 1. RobotState

Represents the current state of the humanoid robot (pose, joint angles, gripper status, etc.)

```yaml
RobotState:
  fields:
    base_pose:
      type: Pose (x, y, z, qx, qy, qz, qw in world frame)
      description: Robot's position and orientation
    joint_angles:
      type: dict[str, float]
      description: Current joint angles (7 for arm, 2 for gripper, 6+ for legs/torso)
      example: {"arm_shoulder_pan": 0.5, "arm_elbow_flex": 1.2, ..., "gripper_left": 0.0, "gripper_right": 0.0}
    gripper_force:
      type: float (Newtons)
      description: Current grip force (0-5N)
    battery_level:
      type: float (0-100%)
      description: Battery state (for real hardware)
    collision_detected:
      type: bool
      description: Emergency stop triggered?
    timestamp:
      type: float (unix timestamp)
      description: When this state was measured
  
  state_machine:
    idle -> walking -> stopped -> grasping -> walking -> idle
    transitions:
      idle: No motion, waiting for commands
      walking: Bipedal locomotion to target location
      stopped: Collision detected or pause command; robot holds current pose
      grasping: Arm executing manipulation (reach, grasp, lift, place)
  
  validation_rules:
    - gripper_force must be in [0, 5]
    - joint_angles must respect mechanical limits (from humanoid.urdf)
    - Only one of {walking, grasping, stopped} can be active at once
  
  update_frequency: 10 Hz (from motor controllers)
```

---

### 2. VoiceCommand

Represents a voice input from the user, transcribed and parsed

```yaml
VoiceCommand:
  fields:
    raw_audio:
      type: binary (WAV or PCM)
      description: Raw microphone input
    transcribed_text:
      type: string
      description: Whisper transcription (e.g., "pick up the cup on the table")
    confidence:
      type: float (0-1)
      description: Transcription confidence score
    parsed_intent:
      type: Intent (see below)
      description: Structured intent derived from text
    timestamp:
      type: float
      description: When command was received
  
  example:
    raw_audio: <100kB WAV file>
    transcribed_text: "pick up the cup on the table"
    confidence: 0.98
    parsed_intent: {action: "grasp", object: "cup", target_location: "table"}
    timestamp: 1701800400.5
```

---

### 3. Intent

Structured representation of user's command; generated by LLM intent parser

```yaml
Intent:
  fields:
    action:
      type: string
      enum: ["navigate", "grasp", "place", "release", "push", "pull", "wait", "clarify"]
      description: High-level action category
    target_object:
      type: string
      description: Object the action is targeting (e.g., "cup", "plate", "door")
    target_location:
      type: string or Pose3D
      description: Destination or place to put object (e.g., "table", "sink", [x, y, z])
    constraints:
      type: dict
      description: Optional constraints (speed, caution, time_limit)
      example: {speed: "slow", caution: true, time_limit_seconds: 30}
    confidence:
      type: float (0-1)
      description: LLM confidence in this parsing
    task_graph:
      type: list[Intent]
      description: For multi-step commands, decompose into subtasks
      example: ["navigate(to=cup)", "grasp(object=cup)", "navigate(to=sink)", "release(object=cup)", "verify(in_sink=true)"]
  
  validation_rules:
    - action must be a valid enum value
    - If action is "grasp", target_object must be specified
    - If action is "navigate", target_location must be specified
    - confidence must be in [0, 1]
  
  example:
    action: "grasp"
    target_object: "cup"
    target_location: "table"
    constraints: {speed: "careful"}
    confidence: 0.95
    task_graph: null  # single-step action
```

---

### 4. ScenePerception

Snapshot of the environment as perceived by the robot

```yaml
ScenePerception:
  fields:
    timestamp:
      type: float
      description: When this scene was captured
    rgb_image:
      type: image (1280x960, RGB)
      description: Color image from RealSense
    depth_image:
      type: image (1280x960, float32 depth in meters)
      description: Depth map from RealSense
    detected_objects:
      type: list[ObjectDetection]
      description: All objects detected in the scene (see below)
    robot_pose:
      type: Pose3D
      description: Robot's pose in world frame (from SLAM)
    occupancy_grid:
      type: OccupancyGrid
      description: 2D map of free/occupied space (from SLAM)
    point_cloud:
      type: list[Point3D]
      description: 3D points from LiDAR or depth map
  
  validation_rules:
    - RGB and depth images must have matching dimensions
    - detected_objects must not be empty (at least one object or empty scene)
    - robot_pose must have valid coordinates
```

---

### 5. ObjectDetection

Individual detected object in the scene

```yaml
ObjectDetection:
  fields:
    object_id:
      type: int
      description: Unique ID for this detection (e.g., 0, 1, 2)
    object_class:
      type: string
      enum: ["cup", "plate", "phone", "book", "ball", "box", "spoon", "knife", "fork", "bottle"]
      description: Object category
    confidence:
      type: float (0-1)
      description: Detection confidence score
    bounding_box_2d:
      type: BBox (x_min, y_min, x_max, y_max in pixels)
      description: 2D bounding box in RGB image
    pose_3d:
      type: Pose3D
      description: 6D pose (position + orientation) in world frame; derived from depth + pose estimation
    pose_error:
      type: float (meters)
      description: Estimated error in pose (±5cm typical)
    occlusion_level:
      type: float (0-1)
      description: Fraction of object occluded (0 = fully visible, 1 = fully occluded)
    grasp_points:
      type: list[Point3D]
      description: Pre-computed grasping points on object (from object database)
  
  validation_rules:
    - confidence must be in [0, 1]
    - occlusion_level must be in [0, 1]
    - pose_3d must be reachable by robot arm (within workspace)
  
  example:
    object_id: 1
    object_class: "cup"
    confidence: 0.92
    bounding_box_2d: {x_min: 400, y_min: 300, x_max: 550, y_max: 450}
    pose_3d: {x: 0.5, y: 0.2, z: 1.0, qx: 0, qy: 0, qz: 0, qw: 1}
    pose_error: 0.03
    occlusion_level: 0.1
    grasp_points: [{x: 0.45, y: 0.15, z: 1.05}, ...]
```

---

### 6. NavPath

Planned navigation trajectory from current position to goal

```yaml
NavPath:
  fields:
    start_pose:
      type: Pose3D
      description: Starting robot pose
    goal_pose:
      type: Pose3D
      description: Target robot pose
    waypoints:
      type: list[Pose3D]
      description: Sequence of intermediate poses for bipedal locomotion
    distance:
      type: float (meters)
      description: Total path length
    duration_estimate:
      type: float (seconds)
      description: Estimated time to traverse path (assuming 0.5 m/s walking speed)
    collision_free:
      type: bool
      description: Whether path has been verified collision-free in occupancy map
    replanned:
      type: bool
      description: Whether this is a replanned path (due to dynamic obstacle)
    timestamp:
      type: float
      description: When path was generated
  
  validation_rules:
    - Start and goal must be distinct
    - All waypoints must be in reachable space (not in obstacles)
    - Distance must be >= 0
    - collision_free must be true before execution
  
  example:
    start_pose: {x: 0.0, y: 0.0, z: 0.0, qx: 0, qy: 0, qz: 0, qw: 1}
    goal_pose: {x: 2.0, y: 1.0, z: 0.0, qx: 0, qy: 0, qz: 0.707, qw: 0.707}
    waypoints: [{...}, {...}, ...]
    distance: 2.5
    duration_estimate: 5.0
    collision_free: true
    replanned: false
```

---

### 7. GraspPlan

Plan for grasping a target object

```yaml
GraspPlan:
  fields:
    target_object:
      type: ObjectDetection
      description: The object to grasp
    candidate_grasps:
      type: list[Grasp]
      description: Multiple grasp candidates ranked by quality
    selected_grasp:
      type: Grasp
      description: The grasp to execute (highest ranked or user-selected)
    gripper_trajectory:
      type: list[Pose3D]
      description: Trajectory of gripper from current pose to grasp pose (via-points)
    joint_trajectory:
      type: list[dict[str, float]]
      description: Joint angles at each point in trajectory (from IK solver)
    duration_estimate:
      type: float (seconds)
      description: Estimated time to execute grasp
    collision_risk:
      type: bool
      description: Whether grasp path intersects with obstacles
  
  validation_rules:
    - target_object must have valid pose_3d
    - selected_grasp must be in candidate_grasps
    - All joint angles must respect mechanical limits
    - collision_risk must be false before execution
```

---

### 8. Grasp

Individual grasp configuration

```yaml
Grasp:
  fields:
    grasp_id:
      type: int
      description: Unique grasp identifier within a plan
    gripper_pose:
      type: Pose3D
      description: Position and orientation of gripper for this grasp
    approach_vector:
      type: Vector3D (unit vector)
      description: Direction of approach (for motion planning)
    stability_metric:
      type: float (0-1)
      description: Grasp quality: higher = more stable (from grasp learning model)
    force_required:
      type: float (Newtons)
      description: Estimated grip force needed to hold object
    collision_free:
      type: bool
      description: Whether this grasp trajectory is collision-free
    rank:
      type: int
      description: Ranking among all candidates (1 = best, higher = worse)
  
  example:
    grasp_id: 0
    gripper_pose: {x: 0.45, y: 0.15, z: 1.0, qx: 0.707, qy: 0, qz: 0, qw: 0.707}
    approach_vector: {x: 0, y: 0, z: -1}
    stability_metric: 0.85
    force_required: 2.5
    collision_free: true
    rank: 1
```

---

### 9. ManipulationTask

Represents a manipulation action: grasp, place, or release

```yaml
ManipulationTask:
  fields:
    task_id:
      type: int
      description: Unique task identifier
    action:
      type: string
      enum: ["grasp", "place", "release", "push", "pull"]
    target_object:
      type: ObjectDetection or string
    source_pose:
      type: Pose3D
      description: Current pose of object (for grasp) or gripper (for place/release)
    target_pose:
      type: Pose3D
      description: Desired pose after action (where to place object)
    grasp_plan:
      type: GraspPlan
      description: For grasp actions, the planned grasp
    status:
      type: string
      enum: ["planned", "executing", "success", "failed", "retrying"]
    retry_count:
      type: int
      description: Number of times this task has been retried (max 3)
    force_feedback:
      type: list[float]
      description: Force measurements during execution (for learning)
  
  validation_rules:
    - retry_count must be <= 3
    - status transitions: planned -> executing -> (success|failed|retrying)
    - If failed and retry_count < 3, can transition back to planned
```

---

### 10. TaskGraph

High-level task decomposition for complex commands

```yaml
TaskGraph:
  fields:
    task_id:
      type: int
      description: Unique task graph identifier
    description:
      type: string
      description: Original command (e.g., "pick up the cup and place it in the sink")
    subtasks:
      type: list[ManipulationTask or NavPath]
      description: Ordered list of subtasks
    dependencies:
      type: dict[int, list[int]]
      description: Task dependencies: task_i depends on tasks in list (for parallelization)
    status:
      type: string
      enum: ["planning", "ready", "executing", "paused", "completed", "failed"]
    completion_percentage:
      type: float (0-100%)
      description: Percentage of subtasks completed
    estimated_duration:
      type: float (seconds)
      description: Total estimated time
  
  example:
    task_id: 10
    description: "pick up the cup and place it in the sink"
    subtasks:
      - type: NavPath, goal: "table location"
      - type: ManipulationTask, action: "grasp", object: "cup"
      - type: NavPath, goal: "sink location"
      - type: ManipulationTask, action: "place", object: "cup", target_location: "sink"
      - type: ManipulationTask, action: "verify", condition: "cup_in_sink"
    dependencies: {0: [], 1: [0], 2: [1], 3: [2], 4: [3]}  # linear chain
    status: "executing"
    completion_percentage: 40
```

---

## Message Types (ROS 2)

Custom ROS 2 message definitions for inter-node communication:

### `humanoid_msgs/ObjectDetectionArray`
```
Header header
ObjectDetectionStamped[] objects
```

### `humanoid_msgs/ObjectDetectionStamped`
```
Header header
string object_class
float32 confidence
float32 pose_x
float32 pose_y
float32 pose_z
float32 pose_qx
float32 pose_qy
float32 pose_qz
float32 pose_qw
float32 grasp_x
float32 grasp_y
float32 grasp_z
```

### `humanoid_msgs/Intent`
```
string action
string target_object
string target_location
string[] constraints
float32 confidence
```

### `humanoid_msgs/RobotState`
```
Header header
geometry_msgs/Pose base_pose
sensor_msgs/JointState joint_angles
float32 gripper_force
bool collision_detected
```

---

## State Transitions & Workflows

### Voice Command → Manipulation Execution

```
1. [Voice Node] → Transcribe audio → Confidence >85% OK
2. [Intent Parser] → Parse text → Generate Intent + TaskGraph
3. [Scene Perception] → Capture RGB-D + SLAM + Object Detection
4. [Navigation Planner] → Plan path to object location
5. [Robot Control] → Execute navigation
6. [Manipulation Planner] → Generate grasps
7. [Robot Control] → Execute grasp
8. [Verification] → Capture post-action scene → Check object in gripper
9. If verification fails → retry grasp (max 3x)
10. If verification succeeds → continue next subtask
```

### Error Handling

```
If [Perception] fails → Ask for clarification, capture new image
If [Planning] fails → Use fallback plan or request human intervention
If [Manipulation] fails after 3 retries → Declare task failed; request human help
If [LLM API] times out → Fall back to local Llama model
```

---

## Data Flow Diagram (ASCII)

```
                    ┌─────────────────┐
                    │  Microphone     │
                    └────────┬────────┘
                             │
                    ┌────────▼────────┐
                    │ Voice Processor │ (Whisper)
                    │   text output   │
                    └────────┬────────┘
                             │
                    ┌────────▼────────┐
                    │  Intent Parser  │ (GPT-4)
                    │ Intent + Tasks  │
                    └─┬──────────┬───┬┘
                      │          │   │
         ┌────────────┘          │   └─────────┐
         │                       │             │
    ┌────▼──────┐        ┌──────▼────┐  ┌────▼──────┐
    │ Navigator │        │Manipulator│  │ Verifier  │
    └────┬──────┘        └──────┬────┘  └────┬──────┘
         │                       │            │
    ┌────▼──────────┐    ┌──────▼────┐  ┌────▼──────┐
    │  ROS 2 Motor  │    │ ROS 2 Arm │  │   Scene   │
    │  Control Node │    │  Control  │  │ Perception│
    └────┬──────────┘    └──────┬────┘  └────┬──────┘
         │                       │            │
    ┌────▼──────────────────────▼────────────▼──────┐
    │         Gazebo Simulation / Real Hardware     │
    │    (Robot, Motors, Gripper, RGB-D, LiDAR)    │
    └─────────────────────────────────────────────┘
         │                                    
         └────────────────┬───────────────────┘
                          │
              ┌───────────▼─────────────┐
              │   ROS 2 Perception Hub  │
              │  (SLAM, Object Detection)
              └─────────────────────────┘
```

---

**Document Status**: ✅ Complete; ready for contract specifications
