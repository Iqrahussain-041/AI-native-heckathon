# Implementation Plan: Autonomous Humanoid Robot Capstone

**Branch**: `001-humanoid-capstone` | **Date**: 2025-12-05 | **Spec**: [specs/001-humanoid-capstone/spec.md](./spec.md)
**Input**: Feature specification from `/specs/001-humanoid-capstone/spec.md`

**Note**: This plan is generated by `/sp.plan` command and provides the technical architecture, design, and phased delivery roadmap for the capstone project.

## Summary

Build a complete autonomous humanoid robot system (simulation-first) that receives natural language voice commands, processes them through intent parsing (LLM), perceives the environment (computer vision + SLAM), plans collision-free paths (ROS 2 Nav2), executes stable grasps (inverse kinematics + manipulation), and closes perception-action loops to verify task success. The system integrates five core Physical AI competencies: voice I/O, scene understanding, autonomous navigation, manipulation, and vision-language-action (VLA) closed loops. Development is phased across 13 weeks with three P1 stories (voice, perception, navigation) as parallel MVP slices, followed by P2 stories (manipulation, VLA integration). All work uses industry-standard tools (ROS 2, Gazebo, NVIDIA Isaac Sim) to ensure sim-to-real transfer capability and deployment to edge hardware (Jetson Orin Nano).

## Technical Context

**Language/Version**: Python 3.10+ (ROS 2 Humble standard)  
**Primary Dependencies**: ROS 2 (DDS middleware), Gazebo 11+, NVIDIA Isaac Sim, OpenAI Whisper, GPT-4 API, OpenCV, scipy, transformers (for LLMs), intel-realsense (SDK)  
**Storage**: File-based (rosbag for logging sensor streams), JSON for config/state, SQLite for optional analytics  
**Testing**: pytest (unit tests), ROS 2 test_communication (integration), manual simulation validation  
**Target Platform**: Ubuntu 22.04 LTS (development on RTX GPU workstation); deployment target: NVIDIA Jetson Orin Nano (8GB)  
**Project Type**: Distributed robotics middleware (ROS 2 nodes across 3-4 processes: perception, cognition, planning, control)  
**Performance Goals**: 
- Voice recognition latency <2s (Whisper transcription)
- Intent parsing latency <3s (GPT-4 API call)
- Object detection inference <500ms per frame (Jetson constraint)
- SLAM mapping <100ms per frame update
- Navigation replanning <1s when obstacle detected
- Grasp execution latency <5s total (plan + motor commands)
- End-to-end task loop (voice → action → verification) <30s simple, <120s complex

**Constraints**: 
- Jetson Orin Nano has only 8GB VRAM; models must be quantized/optimized
- Simulation must be high-fidelity (physics, sensors) to enable sim-to-real transfer
- All code must run on Jetson with <500ms per perception cycle
- ROS 2 latency budget for critical paths <100ms (e.g., collision detection → motor stop)
- LLM API calls must have fallback (local Llama) in case of network failure

**Scale/Scope**: 
- Single humanoid robot in indoor environments (up to 30m × 30m)
- 10 common household objects (cup, plate, phone, book, box, ball, spoon, knife, fork, bottle)
- Single user (voice command from one speaker)
- Capstone: 3 diverse scenarios executed with >80% success

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### Mapped Principles from Constitution

1. **Principle 1: Embodied Excellence** ✅
   - Plan includes mandatory simulation-first pipeline: Gazebo design → algorithm dev → data generation → Jetson deployment → optional physical testing
   - Students master full robotics stack: sensors (RealSense, LiDAR, IMU) → fusion (VSLAM) → cognition (LLM intent parsing) → planning (Nav2 + grasp planning) → control (ROS 2 motor commands) → actuation (movement, grasping) → feedback (verification loops)
   - **Compliance**: Weeks 1-2 (foundations), Weeks 3-13 (hands-on implementation across all stack layers)

2. **Principle 2: Collaborative Intelligence & Code Quality** ✅
   - Plan specifies: original Python code (ROS 2 nodes), peer review gates before merge, git-based version control with meaningful commits
   - Reproducibility requirements: document hardware config (GPU VRAM, CPU), software versions (ROS 2, Python, dependencies in requirements.txt), random seeds, simulation parameters
   - **Compliance**: All source code under version control; tests (pytest, integration) validated before capstone submission; simulation parameters (friction, gravity) documented

3. **Principle 3: Simulation-First Safety & Responsibility** ✅
   - Plan mandates simulation-first: all algorithms tested in Gazebo/Isaac Sim before Jetson deployment (optional physical testing only after faculty safety review)
   - Safety checkpoints: emergency stop button (simulated in Gazebo, physical in lab), collision detection with 0.5m human margin, force/torque feedback for grasp safety
   - Bias testing: all vision models (object detection, pose estimation) tested across lighting conditions and object variations
   - **Compliance**: No real robot deployment without simulation validation; safety scenarios documented; perception metrics validated across demographic variations (if diverse test objects available)

4. **Principle 4: Open Standards & Reproducibility** ✅
   - Plan uses industry-standard tools: ROS 2 (DDS middleware), Gazebo 11, NVIDIA Isaac Sim, OpenCV, transformers library
   - Reproducibility: hardware specs (RTX 4070 Ti, Jetson Orin Nano), software environment locked in requirements.txt, random seeds set for ML models, simulation physics parameters documented
   - **Compliance**: All code open-source compatible; depends on ROS ecosystem packages; deployment documented for reproducibility across machines

5. **Principle 5: Ethical Design & Environmental Stewardship** ✅
   - Design includes failsafes: emergency stop, bounded action spaces (robot halts if object > 2m away), human detection with collision avoidance
   - Environmental stewardship: prefer simulation over real hardware runs; GPU cycles minimized via domain randomization in sim; model quantization (4-bit) for Jetson reduces compute waste
   - **Compliance**: No unnecessary training runs; simulation preferred; quantized models for deployment; transparent decision-making (LLM intent parsing logged)

6. **Principle 6: Continuous Learning & Constitutional Evolution** ✅
   - Plan supports iteration: weekly standup reviews (Weeks 1-13) with progress assessment; capstone demo includes reflection on lessons learned
   - **Compliance**: Weekly milestones enable course-correction; post-capstone analysis documented for future cohorts

### Gate Result: ✅ PASS

All constitutional principles are satisfied. No violations. Plan aligns with program values of embodied excellence, rigorous engineering, simulation-first safety, open standards, ethical responsibility, and continuous learning.

## Project Structure

### Documentation (this feature)

```text
specs/001-humanoid-capstone/
├── plan.md              # This file (/sp.plan command output)
├── research.md          # Phase 0 output (research findings, design decisions)
├── data-model.md        # Phase 1 output (entities, state machines, data flows)
├── quickstart.md        # Phase 1 output (developer setup, first-run instructions)
├── contracts/           # Phase 1 output (ROS 2 topic/service/action schemas)
│   ├── perception-topics.yaml
│   ├── control-topics.yaml
│   ├── planning-services.yaml
│   └── safety-actions.yaml
├── checklists/
│   └── requirements.md   # Specification validation checklist
└── tasks.md             # Phase 2 output (/sp.tasks command - NOT created by /sp.plan)
```

### Source Code (repository root)

**Structure Decision**: Distributed ROS 2 package structure with separate nodes for perception, cognition, planning, and control. This architecture enables:
- Independent development and testing of each node
- Clear separation of concerns (sensor input → processing → motor output)
- Scalability (add new sensors/actuators as packages)
- Sim-to-real transfer (same nodes run in simulation and on Jetson)

```text
src/
├── humanoid_perception/
│   ├── __init__.py
│   ├── voice_processor.py          # Whisper transcription + text-to-speech
│   ├── intent_parser.py            # GPT-4 intent parsing
│   ├── object_detector.py          # Vision: YOLO + 6D pose estimation
│   ├── slam_node.py                # VSLAM: feature tracking + pose estimation
│   └── ros2_nodes/
│       ├── voice_listener_node.py
│       ├── object_detection_node.py
│       └── slam_node.py
├── humanoid_planning/
│   ├── __init__.py
│   ├── navigation_planner.py       # Nav2 integration, path planning
│   ├── grasp_planner.py            # Inverse kinematics, grasp candidate generation
│   ├── task_executor.py            # High-level task decomposition (VLA)
│   └── ros2_nodes/
│       ├── path_planner_node.py
│       ├── grasp_planner_node.py
│       └── task_executor_node.py
├── humanoid_control/
│   ├── __init__.py
│   ├── motor_controller.py         # Motor command interface (simulated + real)
│   ├── force_feedback.py           # Force/torque sensor processing
│   ├── kinematics.py               # Forward & inverse kinematics
│   └── ros2_nodes/
│       └── control_node.py
├── humanoid_common/
│   ├── __init__.py
│   ├── config.py                   # Global config (hardware specs, thresholds)
│   ├── constants.py                # Sensor specs, object database, limits
│   ├── message_types.py            # Custom ROS 2 message definitions
│   └── utils.py                    # Logging, debugging, visualization helpers
└── launch/
    ├── full_system.launch.py       # Launch all nodes (simulation or real)
    ├── perception_only.launch.py   # Dev: test perception stack
    └── navigation_only.launch.py   # Dev: test nav stack

tests/
├── unit/
│   ├── test_voice_processor.py
│   ├── test_intent_parser.py
│   ├── test_object_detector.py
│   ├── test_grasp_planner.py
│   └── test_kinematics.py
├── integration/
│   ├── test_perception_pipeline.py
│   ├── test_planning_pipeline.py
│   ├── test_control_loop.py
│   └── test_full_system.py
└── simulation/
    ├── gazebo_worlds/
    │   ├── simple_room.world
    │   ├── table_with_objects.world
    │   └── complex_environment.world
    └── humanoid_model/
        ├── humanoid.urdf           # Robot kinematics, sensors, actuators
        └── meshes/

data/
├── object_database.json            # 10 known objects: class names, grasping points
├── models/
│   ├── whisper-base.onnx           # Speech recognition (quantized)
│   ├── yolo8-object-detector.pt    # Object detection weights
│   └── pose-estimator-6d.pt        # 6D pose estimation weights
├── calibration/
│   ├── camera_intrinsics.yaml      # RealSense camera calibration
│   └── robot_base_frame.yaml       # Coordinate frame definitions
└── logs/
    └── (auto-generated rosbag files, experiment results)

config/
├── ros2_nodes.yaml                 # Node parameters (topics, service names)
├── robot_specs.yaml                # Hardware: DOF, joint limits, gripper specs
├── simulation_params.yaml          # Gazebo: friction, gravity, timestep
├── perception_thresholds.yaml      # Detection confidence, IK tolerances
└── safety_constraints.yaml         # Collision margins, force limits, speed limits

requirements.txt                    # Python dependencies (exact versions)
setup.py                           # Package installation
README.md                          # Project overview, setup instructions
Makefile                           # Common commands: build, test, run, deploy
```

**Structure Rationale**:
- **Separation by concern**: perception/, planning/, control/ packages enable parallel team work
- **ROS 2 integration**: Each `ros2_nodes/` subdirectory contains runnable nodes
- **Testing**: unit/ (isolated functions), integration/ (node communication), simulation/ (full system in Gazebo)
- **Simulation-ready**: `humanoid_model/`, `gazebo_worlds/`, `simulation_params.yaml` support sim-first development
- **Reproducibility**: `requirements.txt` pins exact versions; `config/` centralizes all tunable parameters; `calibration/` stores sensor configs

## Implementation Phases

### Phase 0: Research & Proof-of-Concept (Weeks 1-2)

**Goals**: Validate core technology choices and resolve architectural unknowns

**Key Tasks**:
- Verify Whisper transcription accuracy on lab microphone + voice samples
- Test GPT-4 API latency and intent parsing reliability on 50 sample commands
- Benchmark object detection (YOLO) on RealSense RGB-D frames; validate ±5cm pose accuracy
- Evaluate VSLAM algorithms (ORB-SLAM2, Cartographer) for Gazebo environments
- Prototype ROS 2 node communication with dummy nodes (no real processing)
- Set up Gazebo 11 environment with humanoid robot model and basic scene

**Outputs**: `research.md` (technology validation, latency measurements, architecture decisions)

**Success Criteria**:
- Whisper transcription: >90% accuracy on clear speech, >85% on noisy
- GPT-4 latency: <3 seconds per intent parsing call (including network)
- YOLO detection: F1 >0.85 on lab dataset (10 objects, various lighting)
- SLAM mapping: <100ms per frame, ±0.2m localization accuracy
- ROS 2 nodes: message passing latency <20ms (DDS middleware)

### Phase 1: MVP Foundation - Perception & Navigation (Weeks 3-8)

**Goals**: Build P1 user stories as independent, deployable modules

**Parallel Track A: Voice & Intent (US1) - Weeks 3-4**
- Implement voice_processor.py (Whisper wrapper with audio input handling)
- Implement intent_parser.py (GPT-4 API with prompt engineering for robot actions)
- Create voice_listener_node.py (ROS 2 node publishing intent messages)
- Unit tests: test_voice_processor.py, test_intent_parser.py
- Demo: "pick up cup" → structured intent `{action: "grasp", object: "cup", location: "table"}`

**Parallel Track B: Perception & Detection (US2) - Weeks 3-5**
- Implement object_detector.py (YOLO + 6D pose estimation)
- Implement object_detection_node.py (ROS 2 publisher on /objects topic)
- Calibrate RealSense camera; store intrinsics in calibration/camera_intrinsics.yaml
- Unit tests: test_object_detector.py
- Gazebo integration: sensor simulator publishing synthetic RGB-D frames
- Demo: camera view of table with cup, plate, phone → outputs `{objects: [{class: "cup", pose: [...]}]}`

**Parallel Track C: SLAM & Navigation (US3) - Weeks 3-7**
- Integrate ROS 2 Nav2 stack with Gazebo LiDAR
- Implement slam_node.py (ORB-SLAM or Cartographer wrapper)
- Implement path_planner_node.py (Nav2 integration for humanoid bipedal paths)
- Unit tests: test_kinematics.py (forward/inverse kinematics for bipedal gait)
- Gazebo integration: LiDAR simulation, physics-based locomotion
- Demo: robot enters room, builds 3D map, localizes to ±0.2m, plans collision-free path to waypoint, executes walk, arrives with <0.1m error

**Integration (Week 8)**:
- Combine A + B + C: voice command → intent → detect object → plan path → navigate
- Integration test: `test_full_system.py` (simulated end-to-end)
- Capstone milestone: Simple task ("go to cup") with >90% success on 5 trials

**Outputs**: 
- humanoid_perception/ package with nodes, tests, Gazebo integration
- humanoid_planning/ package with Nav2 integration
- research.md updated with actual benchmarks
- data-model.md (entities: RobotState, ObjectDetection, NavPath, Intent)
- contracts/perception-topics.yaml, contracts/planning-services.yaml

### Phase 2: Advanced Capabilities - Manipulation & VLA (Weeks 9-12)

**Goals**: Build P2 user stories; integrate voice→action→verification loops

**Track D: Grasp Planning & Manipulation (US4) - Weeks 9-10**
- Implement grasp_planner.py (inverse kinematics + grasp candidate generation)
- Implement humanoid arm model (URDF with 7+ DOF, gripper 2-finger)
- Motor controller interface (motor_controller.py) with Gazebo plugin
- Force/torque sensor feedback (force_feedback.py) for grasp detection
- Unit tests: test_grasp_planner.py, test_kinematics.py
- Gazebo integration: gripper physics, force sensor simulation
- Demo: detect cup → generate 5 grasps → rank by stability → execute grasp → detect grip force >2N → declare success

**Track E: Vision-Language-Action (US5) - Weeks 10-12**
- Implement task_executor.py (task decomposition into subtasks)
- Implement verification loops: execute → sense → compare to goal → retry or succeed
- LLM-based planning (GPT-4) for complex multi-step commands
- Humanoid-specific behaviors: bipedal walking, arm reach, hand manipulation
- Integration tests: `test_control_loop.py`, `test_full_system.py` (complex tasks)
- Demo: "pick up the cup and place it in the sink" → [navigate to cup] → [grasp] → [navigate to sink] → [release] → [verify cup in sink] → success

**Integration (Week 12)**:
- Full pipeline: voice → intent → perception → planning → control → manipulation → verification
- Capstone milestone: Execute 3 different manipulation tasks with >80% success

**Outputs**:
- humanoid_control/ package with motor control, kinematics
- humanoid_planning/ updated with manipulation planning
- contracts/control-topics.yaml, contracts/safety-actions.yaml
- quickstart.md (developer setup: clone, run Gazebo, launch nodes, send commands)
- data-model.md updated with Grasp, ManipulationTask, VLAState entities

### Phase 3: Capstone & Deployment (Week 13)

**Goals**: Execute capstone demo; prepare for real hardware deployment (optional)

**Tasks**:
- 3 diverse capstone scenarios (e.g., "clean table", "fetch object from shelf", "organize items")
- End-to-end system test in Gazebo and Isaac Sim
- Performance profiling on Jetson Orin Nano (optional)
- Documentation: README.md, setup instructions, known limitations
- Optional: Deploy to physical robot (Unitree G1 if available); test sim-to-real transfer
- Capstone presentation: demonstrate 3 scenarios with >80% success

**Success Criteria**:
- All 5 user stories implemented and tested
- >90% success on simple tasks (voice → perception → navigation)
- >80% success on complex tasks (grasp + place, multi-step commands)
- System deployable to Jetson Orin Nano with <500ms perception latency
- Code reproducible: another student can run it with <10 lines of config changes

---

## Complexity Tracking

**Constitutional Violations**: ✅ NONE (see Constitution Check above)

**Technical Debt & Mitigations**:
1. LLM API dependency: fallback to local Llama for offline inference
2. Sim-to-real gap: use domain randomization in Gazebo; plan hardware testing phase post-capstone
3. Model quantization: 4-bit quantization for Jetson; trade-off: slight accuracy loss (~2-3%) for 4× speedup
4. Single robot, no multi-agent: sufficient for capstone; extensible in future cohorts

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| [e.g., 4th project] | [current need] | [why 3 projects insufficient] |
| [e.g., Repository pattern] | [specific problem] | [why direct DB access insufficient] |
